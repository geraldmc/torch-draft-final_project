{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geraldmc/torch-draft-final_project/blob/main/lr_sched_early_stop_pyt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLikHUYXKweW"
      },
      "source": [
        "## Prepare the Python Scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBuoZ1PgK01f",
        "outputId": "a33947fd-bed5-47b1-a5c1-abaa1db777a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils.py\n",
        "import torch\n",
        "\n",
        "class LRScheduler():\n",
        "    \"\"\"\n",
        "    Learning rate scheduler. If the validation loss does not decrease for the \n",
        "    given number of `patience` epochs, then the learning rate will decrease by\n",
        "    by given `factor`.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, optimizer, patience=5, min_lr=1e-6, factor=0.5\n",
        "    ):\n",
        "        \"\"\"\n",
        "        new_lr = old_lr * factor\n",
        "\n",
        "        :param optimizer: the optimizer we are using\n",
        "        :param patience: how many epochs to wait before updating the lr\n",
        "        :param min_lr: least lr value to reduce to while updating\n",
        "        :param factor: factor by which the lr should be updated\n",
        "        \"\"\"\n",
        "        self.optimizer = optimizer\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.factor = factor\n",
        "\n",
        "        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
        "                self.optimizer,\n",
        "                mode='min',\n",
        "                patience=self.patience,\n",
        "                factor=self.factor,\n",
        "                min_lr=self.min_lr,\n",
        "                verbose=True\n",
        "            )\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        self.lr_scheduler.step(val_loss)\n",
        "\n",
        "class EarlyStopping():\n",
        "    \"\"\"\n",
        "    Early stopping to stop the training when the loss does not improve after\n",
        "    certain epochs.\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        \"\"\"\n",
        "        :param patience: how many epochs to wait before stopping when loss is\n",
        "               not improving\n",
        "        :param min_delta: minimum difference between new loss and old loss for\n",
        "               new loss to be considered as an improvement\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss == None:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            # reset counter if validation loss improves\n",
        "            self.counter = 0\n",
        "        elif self.best_loss - val_loss < self.min_delta:\n",
        "            self.counter += 1\n",
        "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                print('INFO: Early stopping')\n",
        "                self.early_stop = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWjdPAHRK4x-",
        "outputId": "0138e54a-f2d7-4fb5-ff1a-cd7ef59b18ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing dataset.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile dataset.py\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "# define the image transforms and augmentations\n",
        "train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "# traning and validation datasets and dataloaders\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    root='../input/alien_vs_predator_thumbnails/data/train',\n",
        "    transform=train_transform\n",
        ")\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=32, shuffle=True,\n",
        ")\n",
        "val_dataset = datasets.ImageFolder(\n",
        "    root='../input/alien_vs_predator_thumbnails/data/validation',\n",
        "    transform=val_transform\n",
        ")\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=32, shuffle=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRoSfbCjMAmI",
        "outputId": "e8d42c77-e5be-4987-e835-8dff316a8313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing models.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile models.py\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "def resnet50(pretrained=True, requires_grad=False):\n",
        "    model = models.resnet50(progress=True, pretrained=pretrained)\n",
        "    # either freeze or train the hidden layer parameters\n",
        "    if requires_grad == False:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "    elif requires_grad == True:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "    # make the classification layer learnable\n",
        "    model.fc = nn.Linear(2048, 2)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiKkR0noMV1H",
        "outputId": "386d203e-f1cc-4f18-c17f-11a4eb1386cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import models\n",
        "import argparse\n",
        "\n",
        "from dataset import train_dataloader, val_dataloader\n",
        "from dataset import train_dataset, val_dataset\n",
        "from utils import EarlyStopping, LRScheduler\n",
        "from tqdm import tqdm\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "# construct the argument parser\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--lr-scheduler', dest='lr_scheduler', action='store_true')\n",
        "parser.add_argument('--early-stopping', dest='early_stopping', action='store_true')\n",
        "args = vars(parser.parse_args())\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Computation device: {device}\\n\")\n",
        "# instantiate the model\n",
        "model = models.resnet50(pretrained=True, requires_grad=False).to(device)\n",
        "# total parameters and trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} training parameters.\")\n",
        "\n",
        "# learning parameters \n",
        "lr = 0.001\n",
        "epochs = 100\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# strings to save the loss plot, accuracy plot, and model with different ...\n",
        "# ... names according to the training type\n",
        "# if not using `--lr-scheduler` or `--early-stopping`, then use simple names\n",
        "loss_plot_name = 'loss'\n",
        "acc_plot_name = 'accuracy'\n",
        "model_name = 'model'\n",
        "\n",
        "# either initialize early stopping or learning rate scheduler\n",
        "if args['lr_scheduler']:\n",
        "    print('INFO: Initializing learning rate scheduler')\n",
        "    lr_scheduler = LRScheduler(optimizer)\n",
        "    # change the accuracy, loss plot names and model name\n",
        "    loss_plot_name = 'lrs_loss'\n",
        "    acc_plot_name = 'lrs_accuracy'\n",
        "    model_name = 'lrs_model'\n",
        "if args['early_stopping']:\n",
        "    print('INFO: Initializing early stopping')\n",
        "    early_stopping = EarlyStopping()\n",
        "    # change the accuracy, loss plot names and model name\n",
        "    loss_plot_name = 'es_loss'\n",
        "    acc_plot_name = 'es_accuracy'\n",
        "    model_name = 'es_model'\n",
        "\n",
        "# training function\n",
        "def fit(model, train_dataloader, train_dataset, optimizer, criterion):\n",
        "    print('Training')\n",
        "    model.train()\n",
        "    train_running_loss = 0.0\n",
        "    train_running_correct = 0\n",
        "    counter = 0\n",
        "    total = 0\n",
        "    prog_bar = tqdm(enumerate(train_dataloader), total=int(len(train_dataset)/train_dataloader.batch_size))\n",
        "    for i, data in prog_bar:\n",
        "        counter += 1\n",
        "        data, target = data[0].to(device), data[1].to(device)\n",
        "        total += target.size(0)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        train_running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        train_running_correct += (preds == target).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    train_loss = train_running_loss / counter\n",
        "    train_accuracy = 100. * train_running_correct / total\n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "# validation function\n",
        "def validate(model, test_dataloader, val_dataset, criterion):\n",
        "    print('Validating')\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    val_running_correct = 0\n",
        "    counter = 0\n",
        "    total = 0\n",
        "    prog_bar = tqdm(enumerate(test_dataloader), total=int(len(val_dataset)/test_dataloader.batch_size))\n",
        "    with torch.no_grad():\n",
        "        for i, data in prog_bar:\n",
        "            counter += 1\n",
        "            data, target = data[0].to(device), data[1].to(device)\n",
        "            total += target.size(0)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            \n",
        "            val_running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            val_running_correct += (preds == target).sum().item()\n",
        "        \n",
        "        val_loss = val_running_loss / counter\n",
        "        val_accuracy = 100. * val_running_correct / total\n",
        "        return val_loss, val_accuracy\n",
        "\n",
        "# lists to store per-epoch loss and accuracy values\n",
        "train_loss, train_accuracy = [], []\n",
        "val_loss, val_accuracy = [], []\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "    train_epoch_loss, train_epoch_accuracy = fit(\n",
        "        model, train_dataloader, train_dataset, optimizer, criterion\n",
        "    )\n",
        "    val_epoch_loss, val_epoch_accuracy = validate(\n",
        "        model, val_dataloader, val_dataset, criterion\n",
        "    )\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    train_accuracy.append(train_epoch_accuracy)\n",
        "    val_loss.append(val_epoch_loss)\n",
        "    val_accuracy.append(val_epoch_accuracy)\n",
        "    if args['lr_scheduler']:\n",
        "        lr_scheduler(val_epoch_loss)\n",
        "    if args['early_stopping']:\n",
        "        early_stopping(val_epoch_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            break\n",
        "\n",
        "    print(f\"Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_accuracy:.2f}\")\n",
        "    print(f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_accuracy:.2f}')\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Training time: {(end-start)/60:.3f} minutes\")\n",
        "\n",
        "print('Saving loss and accuracy plots...')\n",
        "# accuracy plots\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_accuracy, color='green', label='train accuracy')\n",
        "plt.plot(val_accuracy, color='blue', label='validataion accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.savefig(f\"../outputs/{acc_plot_name}.png\")\n",
        "plt.show()\n",
        "\n",
        "# loss plots\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_loss, color='orange', label='train loss')\n",
        "plt.plot(val_loss, color='red', label='validataion loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig(f\"../outputs/{loss_plot_name}.png\")\n",
        "plt.show()\n",
        "    \n",
        "# serialize the model to disk\n",
        "print('Saving model...')\n",
        "torch.save(model.state_dict(), f\"../outputs/{model_name}.pth\")\n",
        " \n",
        "print('TRAINING COMPLETE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77pR6lCsT-nD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "lr_sched_early_stop_pyt.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
